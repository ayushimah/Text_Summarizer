{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization in Python\n",
    "\n",
    "## Motivation: \n",
    "The length of textual data is increasing and people have less time. Often the newspaper articles run into a long text of, say 1000 -1200 words. As wearable devices leap to prominence (Google Glass, Apple Watch, to name a few), content must adapt to the limited screen space available on these devices.\n",
    "The task of generating intelligent and accurate summaries for long pieces of text has become a popular research as well as industry problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach: \n",
    "Extractive text summarization is all about finding the more important sentences from a document as a summary of that document.\n",
    "Our approach is using the TextRank algorithm to find these 'important' sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1. Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numpy library helps in working with arrays: array creation and manipulation\n",
    "# this implementation uses array for storing the matrices generated as 2-D arrays\n",
    "# PyPDF2 is a library used for reading the PDF files\n",
    "# sys library has been used for printing the size of data structures used in the program\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matplotlib is a library that is used to visualize the data by drawing graphs of matrix inputs\n",
    "# we will use it for drawing the matrices generated later in the program \n",
    "# %matplotlib inline is a command used to show the graphs in the jupyter notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# networkx library helps in working with graphs ...\n",
    "# and later performing the PageRank algorithm ...\n",
    "# which is the crux of this implementation to find ...\n",
    "# the importance of each sentence using their 'rank' as a metric ...\n",
    "# rank, the output of the method textrank, is a measure of importance of sentences\n",
    "# this library has been used in the cell no. ()\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the PunktSentenceTokenizer library is being imported from the file punkt.py contained in package nltk.tokenize \n",
    "# this is used to tokenize the document into sentences\n",
    "\n",
    "# Tokenization: Tokenization is the process of demarcating and possibly classifying.. \n",
    "# sections of a string of input characters. \n",
    "# The resulting tokens are then passed on to some other form of processing. \n",
    "\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TfidfTransformer and CountVectorizer libraries are being imported\n",
    "\n",
    "# CountVectorizer: In this implementation, a CountVectorizer object is being created that ..\n",
    "# will be used for creating the document-term matrix\n",
    "\n",
    "# tFidTransformer: In this implementation,TfidfTransformer is used for executing the method fit_transform()... \n",
    "# which provides the output as a document-term matrix normalized (value 0-1) according to the TF-IDF\n",
    "# TF(Term Frequency): the no. of times a term(a word here) appears in the current document(single sentence here)\n",
    "# IDF(Inverse Document Frequency): the no. of times a term(a word here) appears in the entire corpus\n",
    "# Corpus: set of all sentences\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Function to read the document from user\n",
    "Supported formats: .txt, .pdf \n",
    "\n",
    "Input: Takes the name of the file as input. \n",
    "\n",
    "Output: Returns a string output containing the contents of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take the document as an input\n",
    "def readDoc():\n",
    "    name = input('Please input a file name: ') \n",
    "    print('You have asked for the document {}'.format(name))\n",
    "\n",
    "    # now read the type of document\n",
    "    if name.lower().endswith('.txt'):\n",
    "        choice = 1\n",
    "    elif name.lower().endswith('.pdf'):\n",
    "        choice = 2\n",
    "   \n",
    "    # Case 1: if it is a .txt file\n",
    "        \n",
    "    if choice == 1:\n",
    "        f = open(name, 'r')\n",
    "        document = f.read()\n",
    "        f.close()\n",
    "            \n",
    "    # Case 2: if it is a .pdf file\n",
    "    \n",
    "    elif choice == 2:\n",
    "        pdfFileObj = open(name, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        pageObj = pdfReader.getPage(0)\n",
    "        document = pageObj.extractText()\n",
    "        pdfFileObj.close()\n",
    "    \n",
    "    # Case 3: none of the format\n",
    "    else:\n",
    "        print('Failed to load a valid file')\n",
    "        print('Returning an empty string')\n",
    "        document = ''\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Function to tokenize the document\n",
    "Input: String of text document\n",
    "\n",
    "Output: A list containing sentences as its elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the function used for tokenizing the sentences\n",
    "\n",
    "def tokenize(document):\n",
    "    # We are tokenizing using the PunktSentenceTokenizer\n",
    "    # we call an instance of this class as doc_tokenizer\n",
    "    doc_tokenizer = PunktSentenceTokenizer()\n",
    "    \n",
    "    # tokenize() method: takes our document as input \n",
    "    # and returns a list of all the sentences in the document\n",
    "    \n",
    "    # sentences_list is a list containing each sentence of the document as an element\n",
    "    sentences_list = doc_tokenizer.tokenize(document)\n",
    "    print(type(sentences_list))\n",
    "    return sentences_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Read the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input a file name: Small_doc2.txt\n",
      "You have asked for the document Small_doc2.txt\n",
      "The size of the file is: 87\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# reading a file and \n",
    "# printing the size of the file\n",
    "document = readDoc()\n",
    "print('The size of the file is: {}'.format(sys.getsizeof(document)))\n",
    "print(type(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate a list of sentences in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "The size of the list in Bytes is: 104\n",
      "The size of the item 0 in Bytes is: 67\n"
     ]
    }
   ],
   "source": [
    "# we want to tokenize the document for further processing\n",
    "# tokenizing the sentence means that we are creating a list of all the sentences of the document.\n",
    "# Need of tokenizing the document: Initially the document is in just a string format.\n",
    "# if we want to process the document, we need to store it in a data structure.\n",
    "# Tokenization of document into words is also possible, but we will go with the tokenizing with the sentences\n",
    "# Since we want to choose the most relevant sentences, we need to generate tokens of sentences only\n",
    "\n",
    "sentences_list = tokenize(document)\n",
    "#for i in sentences_list:\n",
    "    \n",
    "# let us print the size of memory used by the list sentences\n",
    "print('The size of the list in Bytes is: {}'.format(sys.getsizeof(sentences_list)))\n",
    "\n",
    "# the size of one of the element of the list\n",
    "print('The size of the item 0 in Bytes is: {}'.format(sys.getsizeof(sentences_list[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# let us see the data type of sentences_list\n",
    "# It will be list\n",
    "print(type(sentences_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run for your life.\n",
      "Life is beautiful!\n"
     ]
    }
   ],
   "source": [
    "# print the elements of the list\n",
    "# If the input document is long, which on realistically will be wrong, we would not like to print the entire document\n",
    "for i in sentences_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Generate term-document matrix (TD matrix) of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit_transform method of CountVectorizer() class \n",
    "# Learn the vocabulary dictionary and return term-document matrix. \n",
    "# I/p: list of sentences\n",
    "# O/p: The term-document matrix named cv_matrix\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "cv_matrix = cv.fit_transform(sentences_list)\n",
    "\n",
    "# list of stopwords: https://gist.github.com/sebleier/554280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So what does CountVectorizer.fit_transform() do?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result demo array is [[1 1 1]\n",
      " [0 1 0]]\n",
      "Feature list: ['ashish', 'bad', 'good']\n",
      "\n",
      "\n",
      "[[0.6316672  0.44943642 0.6316672 ]\n",
      " [0.         1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# a demo of what CountVectorizer().fit_transform(text) does\n",
    "cv_demo = CountVectorizer(stop_words='english') # a demo object of class CountVectorizer\n",
    "\n",
    "# I have repeated the words to make a non-ambiguous array of the document text matrix \n",
    "\n",
    "text_demo = [\"Ashish is good, you are bad\", \"I always not bad\"] \n",
    "res_demo = cv_demo.fit_transform(text_demo)\n",
    "print('Result demo array is {}'.format(res_demo.toarray()))\n",
    "\n",
    "# Result is 2-d matrix containing document text matrix\n",
    "# Notice that in the second row, there is 2.\n",
    "# also, bad is repeated twice in that sentence.\n",
    "# so we can infer that 2 is corresponding to the word 'bad'\n",
    "print('Feature list: {}'.format(cv_demo.get_feature_names()))\n",
    "print('')\n",
    "print('')\n",
    "normal_matrix = TfidfTransformer().fit_transform(res_demo)\n",
    "print(normal_matrix.toarray())\n",
    "\n",
    "# why no I?\n",
    "# ans: https://stackoverflow.com/questions/20717641/countvectorizer-i-not-showing-up-in-vectorized-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data type of bow matrix <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Shape of the matrix <bound method spmatrix.get_shape of <2x3 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 4 stored elements in Compressed Sparse Row format>>\n",
      "Size of the matrix is: 56\n",
      "['beautiful', 'life', 'run']\n",
      "[[0 1 1]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# printing the cv_matrix type\n",
    "# and how it is being stored in memory?\n",
    "# it is stored in the compressed row format\n",
    "# compressed row format: \n",
    "print('The data type of bow matrix {}'.format(type(cv_matrix)))\n",
    "print('Shape of the matrix {}'.format(cv_matrix.get_shape))\n",
    "print('Size of the matrix is: {}'.format(sys.getsizeof(cv_matrix)))\n",
    "print(cv.get_feature_names())\n",
    "print(cv_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.57973867 0.81480247]\n",
      " [0.81480247 0.57973867 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Tnormalized: document-term matrix normalized (value 0-1) according to the TF-IDF\n",
    "# TF(Term Frequency): the no. of times a term(a word here) appears in the current document(single sentence here)\n",
    "# IDF(Inverse Document Frequency): the no. of times a term(a word here) appears in the entire corpus\n",
    "# Corpus: set of all sentences\n",
    "\n",
    "\n",
    "\n",
    "normal_matrix = TfidfTransformer().fit_transform(cv_matrix)\n",
    "print(normal_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method _cs_matrix.toarray of <3x2 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 4 stored elements in Compressed Sparse Column format>>\n"
     ]
    }
   ],
   "source": [
    "print(normal_matrix.T.toarray)\n",
    "res_graph = normal_matrix * normal_matrix.T\n",
    "# plt.spy(res_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges 3\n",
      "Number of vertices 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashish/anaconda3/lib/python3.6/site-packages/networkx/drawing/nx_pylab.py:126: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "  b = plt.ishold()\n",
      "/home/ashish/anaconda3/lib/python3.6/site-packages/networkx/drawing/nx_pylab.py:522: MatplotlibDeprecationWarning: The is_string_like function was deprecated in version 2.1.\n",
      "  if not cb.is_string_like(edge_color) \\\n",
      "/home/ashish/anaconda3/lib/python3.6/site-packages/networkx/drawing/nx_pylab.py:543: MatplotlibDeprecationWarning: The is_string_like function was deprecated in version 2.1.\n",
      "  if cb.is_string_like(edge_color) or len(edge_color) == 1:\n",
      "/home/ashish/anaconda3/lib/python3.6/site-packages/networkx/drawing/nx_pylab.py:138: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "  plt.hold(b)\n",
      "/home/ashish/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:911: MatplotlibDeprecationWarning: axes.hold is deprecated. Please remove it from your matplotlibrc and/or style files.\n",
      "  mplDeprecation)\n",
      "/home/ashish/anaconda3/lib/python3.6/site-packages/matplotlib/rcsetup.py:156: MatplotlibDeprecationWarning: axes.hold is deprecated, will be removed in 3.0\n",
      "  mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFCCAYAAADL3BUJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory used by the graph in Bytes is: 56\n"
     ]
    }
   ],
   "source": [
    "# drawing a graph to proceed for the textrank algorithm\n",
    "# nx_graph is a graph developed using the networkx library\n",
    "# each node represents a sentence\n",
    "# an edge represents that they have words in common\n",
    "# the edge weight is the number of words that are common in both of the sentences(nodes)\n",
    "# nx.draw() method is used to draw the graph created\n",
    "\n",
    "nx_graph = nx.from_scipy_sparse_matrix(res_graph)\n",
    "nx.draw_circular(nx_graph)\n",
    "print('Number of edges {}'.format(nx_graph.number_of_edges()))\n",
    "print('Number of vertices {}'.format(nx_graph.number_of_nodes()))\n",
    "plt.show()\n",
    "print('The memory used by the graph in Bytes is: {}'.format(sys.getsizeof(nx_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  note that the graph above is dense and therefor it resembles a circle\n",
    "# if a shorter document is taken, a beautiful circular graph can be seen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Getting the rank of every sentence using textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "The size used by the dictionary in Bytes is: 240\n",
      "0 0.49999999999999994\n",
      "1 0.49999999999999994\n"
     ]
    }
   ],
   "source": [
    "# ranks is a dictionary with key=node(sentences) and value=textrank (the rank of each of the sentences)\n",
    "# http://www.aclweb.org/anthology/W04-3252\n",
    "# the use of d is  L is a damping factor that can be set between\n",
    "# 0 and 1, which has the role of integrating into the\n",
    "# model the probability of jumping from a given vertex\n",
    "# to another random vertex in the grap\n",
    "\n",
    "ranks = nx.pagerank(nx_graph)\n",
    "\n",
    "# analyse the data type of ranks\n",
    "print(type(ranks))\n",
    "print('The size used by the dictionary in Bytes is: {}'.format(sys.getsizeof(ranks)))\n",
    "\n",
    "# print the dictionary\n",
    "for i in ranks:\n",
    "    print(i, ranks[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Finding important sentences and generating summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enumerate method: returns an enumerate object\n",
    "# Use of list Comprehensions\n",
    "# O/p: sentence_array is the sorted(descending order w.r.t. score value) 2-d array of ranks[sentence] and sentence \n",
    "# For example, if there are two sentences: S1 (with a score of S1 = s1) and S2 with score s2, with s2>s1\n",
    "# then sentence_array is [[s2, S2], [s1, S1]]\n",
    "sentence_array = sorted(((ranks[i], s) for i, s in enumerate(sentences_list)), reverse=True)\n",
    "sentence_array = np.asarray(sentence_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as sentence_array is in descending order wrt score value\n",
    "# fmax is the largest score value(the score of first element)\n",
    "# fmin is the smallest score value(the score of last element)\n",
    "\n",
    "rank_max = float(sentence_array[0][0])\n",
    "rank_min = float(sentence_array[len(sentence_array) - 1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49999999999999994\n",
      "0.49999999999999994\n"
     ]
    }
   ],
   "source": [
    "# print the largest and smallest value of scores of the sentence\n",
    "print(rank_max)\n",
    "print(rank_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Normalization of the scores\n",
    "# so that it comes out in the range 0-1\n",
    "# fmax becomes 1\n",
    "# fmin becomes 0\n",
    "# store the normalized values in the list temp_array\n",
    "\n",
    "temp_array = []\n",
    "\n",
    "# if all sentences have equal ranks, means they are all the same\n",
    "# taking any sentence will give the summary, say the first sentence\n",
    "flag = 0\n",
    "if rank_max - rank_min == 0:\n",
    "    temp_array.append(0)\n",
    "    flag = 1\n",
    "\n",
    "# If the sentence has different ranks\n",
    "if flag != 1:\n",
    "    for i in range(0, len(sentence_array)):\n",
    "        temp_array.append((float(sentence_array[i][0]) - rank_min) / (rank_max - rank_min))\n",
    "\n",
    "print((temp_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculation of threshold:\n",
    "# We take the mean value of normalized scores\n",
    "# any sentence with the normalized score 0.2 more than the mean value is considered to be \n",
    "threshold = (sum(temp_array) / len(temp_array)) + 0.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate out the sentences that satiasfy the criteria of having a score above the threshold\n",
    "sentence_list = []\n",
    "if len(temp_array) > 1:\n",
    "    for i in range(0, len(temp_array)):\n",
    "        if temp_array[i] > threshold:\n",
    "            sentence_list.append(sentence_array[i][1])\n",
    "else:\n",
    "    sentence_list.append(sentence_array[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Writing the summary to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run for your life.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(sentence_list)\n",
    "summary = \" \".join(str(x) for x in sentence_list)\n",
    "print(summary)\n",
    "# save the data in another file, names sum.txt\n",
    "f = open('p.txt', 'a+')\n",
    "#print(type(f))\n",
    "f.write('\\n')\n",
    "f.write(summary)\n",
    "f.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run for your life.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for lines in sentence_list:\n",
    "    print(lines)\n",
    "print(len(sentence_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please feel free to contribue for any improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
